
# vvvvvvvvvv Don't forget to change this for new experiments vvvvvvvvvv
type: vae_trainer.VAETrainer  # Used to import the trainer from this class

# This config is similar to the hyperparameters of GPT-3 Medium which had 350M parameters.
# (assuming sequence lengths of 1024)

# total batch size across all devices
global_batch_size: 512

# run for N steps
max_steps: 600000

# Save a checkpoint every N steps
checkpoint_interval: 10000


# whether to autocast the forward pass and loss computation.
# currently only supports autocasting to bfloat16
use_autocast: true


# optimizer configuration
optimizer:

  # Used to import the optimizer from this class.
  type: adamw.AdamW # GPT-3 actually used Adam

  # kwargs passed to the optimizer constructor
  kwargs:

    lr: 3.0e-4

    betas: [0.9, 0.95]
    
    eps: 1e-6 # GPT-3 used 1e-8

    weight_decay: 0.1


# Learning rate scheduler configuration
lr_scheduler:

  # name of the scheduler
  # see  https://huggingface.co/docs/transformers/main/en/main_classes/optimizer_schedules#transformers.get_scheduler
  type: cosine_with_min_lr

  # get_scheduler requires this outside of kwargs
  num_warmup_steps: 750

  # cosine scheduler requires this outside of kwargs
  num_training_steps: 520000

  # kwargs passed to the scheduler constructor
  kwargs:

    min_lr_rate: 0.1


# Defaults to clip the L2 norm of gradients to 1.0.
# Set to null to disable gradient clipping by norm.
max_grad_norm: 1.0
# Defaults to not clip gradients by their absolute value.
# Set to a number to clip gradients by the specified max absolute value.
max_grad_value: null


# You can add any additional configuration options for your trainer here
# They will appear as attributes of config.trainer

kl_weight: 0.1
