
# vvvvvvvvvv Don't forget to change this for new experiments vvvvvvvvvv
type: lm_trainer.LMTrainer  # Used to import the trainer from this class

# This config is similar to the hyperparameters of GPT-3 XL which had 1.3B parameters.
# (assuming sequence lengths of 1024)

# total batch size across all devices
global_batch_size: 1024

# run for N steps
max_steps: 300000

# Save a checkpoint every N steps
checkpoint_interval: 10000


# optimizer configuration
optimizer:

  # Used to import the optimizer from this class.
  type: adamw.AdamW # GPT-3 actually used Adam

  # kwargs passed to the optimizer constructor
  kwargs:

    lr: 1.0e-4

    betas: [0.9, 0.95]
    
    eps: 1e-6 # GPT-3 used 1e-8

    weight_decay: 0.1


# Learning rate scheduler configuration
lr_scheduler:

  # name of the scheduler
  # see  https://huggingface.co/docs/transformers/main/en/main_classes/optimizer_schedules#transformers.get_scheduler
  type: cosine_with_min_lr

  # get_scheduler requires this outside of kwargs
  num_warmup_steps: 375

  # kwargs passed to the scheduler constructor
  kwargs:

    num_training_steps: 300000 # end of training unlike GPT-3 which decayed to 0 over 260B tokens
    
    min_lr_rate: 0.1
  

# Defaults to clip the L2 norm of gradients to 1.0.
# Set to null to disable gradient clipping by norm.
max_grad_norm: 1.0
# Defaults to not clip gradients by their absolute value.
# Set to a number to clip gradients by the specified max absolute value.
max_grad_value: null


# You can add any additional configuration options for your trainer here
# They will appear as attributes of config.trainer
