defaults:
  - _self_  # refers to this config file
  - sharding: llama-fsdp  # refers to sharding/llama-fsdp.yaml
  - remat: llama-scan  # refers to remat/llama-scan.yaml


# Used to import the model from this class
# should contain the path within the model folder and the class name
type: llama.LlamaForCausalLM

# HuggingFace URL of the checkpoint to load, or null if initializing from scratch
pretrained_url: null
# Step of the checkpoint to load
pretrained_step: null
# Whether to strictly enforce that the keys in the checkpoint match the model state dict
pretrained_strict: null

# dtype of the model parameters. Should probably be float32
torch_dtype: float32

# vocab information
vocab_size: 32000
bos_token_id: 1
eos_token_id: 2
pad_token_id: 32000 # not actually in the embeddings

# architecture size
hidden_size: 2048
num_hidden_layers: 22

# attention configuration
num_attention_heads: 32
num_key_value_heads: 4

# MLP configuration
intermediate_size: 5632
hidden_act: silu

# rope configuration
max_position_embeddings: 2048
rope_theta: 10000.0

# initialization
initializer_range: 0.02

# misc
attention_dropout: false
attention_bias: false
rms_norm_eps: 1.0e-05

# choose attention_kernel from: [flash_attention, nan_safe_flash_attention, splash_attention, null]
attention_kernel: flash_attention

# List of names of classes in the module tree that are functionally pure.
#
# There are a few advantages of wrapping a module whose forward pass you know is
# free of side-effects and whose behavior only depends on inputs in a `PureModule`:
# - `PureModule`s will only be traced once.
# - Framework profile scopes added via `xp.Trace` will show up in both the forward
#   and the backward pass.
#
# Kernel attention implementations cannot be wrapped in a `PureModule`
#
# Note: This is not currently supported for PyTorch/XLA 2.8.0
pure_modules: []
# - LlamaMLP
