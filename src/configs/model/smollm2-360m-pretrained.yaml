defaults:
  - _self_  # refers to this config file
  - sharding: llama-fsdp  # refers to sharding/llama-fsdp.yaml
  - remat: llama-scan  # refers to remat/llama-scan.yaml


# Used to import the model from this class
# should contain the path within the model folder and the class name
type: llama.LlamaForCausalLM

# This checkpoint was created using scripts/convert_hf_checkpoint.py
pretrained_url: aklein4/SmolLM2-360M-TPU
pretrained_step: 0
pretrained_strict: true

# dtype of the model parameters. Should probably be float32
torch_dtype: float32

# vocab information
vocab_size: 49152
bos_token_id: 0
eos_token_id: 0
pad_token_id: 49152 # not actually in the embeddings

# architecture size
hidden_size: 960
num_hidden_layers: 32

# attention configuration
num_attention_heads: 15
num_key_value_heads: 5

# MLP configuration
intermediate_size: 2560
hidden_act: silu

# rope configuration
max_position_embeddings: 8192
rope_theta: 100000

# misc
attention_dropout: false
attention_bias: false
initializer_range: 0.02
rms_norm_eps: 1.0e-05

# choose attention_kernel from: [flash_attention, nan_safe_flash_attention, splash_attention, null]
attention_kernel: flash_attention

# List of names of classes in the module tree that are functionally pure.
#
# There are a few advantages of wrapping a module whose forward pass you know is
# free of side-effects and whose behavior only depends on inputs in a `PureModule`:
# - `PureModule`s will only be traced once.
# - Framework profile scopes added via `xp.Trace` will show up in both the forward
#   and the backward pass.
#
# Kernel attention implementations cannot be wrapped in a `PureModule`
pure_modules:
 - LlamaMLP
 