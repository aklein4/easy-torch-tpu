# See docs/sharding.md for more details about sharding configuration.

# Weights
embed_tokens.weight: [fsdp, null]
decoder_encoder_model.lm_head.weight: [fsdp, null]


encoder_model.layers.*.self_attn.q_proj.weight: [fsdp, null]
encoder_model.layers.*.self_attn.k_proj.weight: [null, fsdp]
encoder_model.layers.*.self_attn.v_proj.weight: [null, fsdp]
encoder_model.layers.*.self_attn.o_proj.weight: [fsdp, null]

encoder_model.layers.*.mlp.gate_proj.weight: [fsdp, null]
encoder_model.layers.*.mlp.up_proj.weight: [fsdp, null]
encoder_model.layers.*.mlp.down_proj.weight: [null, fsdp]

encoder_model.layers.*.input_layernorm.weight: [fsdp]
encoder_model.layers.*.post_attention_layernorm.weight: [fsdp]
encoder_model.norm.weight: [fsdp]


decoder_model.layers.*.self_attn.q_proj.weight: [fsdp, null]
decoder_model.layers.*.self_attn.k_proj.weight: [null, fsdp]
decoder_model.layers.*.self_attn.v_proj.weight: [null, fsdp]
decoder_model.layers.*.self_attn.o_proj.weight: [fsdp, null]

decoder_model.layers.*.mlp.gate_proj.weight: [fsdp, null]
decoder_model.layers.*.mlp.up_proj.weight: [fsdp, null]
decoder_model.layers.*.mlp.down_proj.weight: [null, fsdp]

decoder_model.layers.*.input_layernorm.weight: [fsdp]
decoder_model.layers.*.post_attention_layernorm.weight: [fsdp]
decoder_model.norm.weight: [fsdp]


# Activations
encoder_model.layers.*: [[data, fsdp], null, null]
decoder_model.layers.*: [[data, fsdp], null, null]
decoder_model.lm_head: [[data, fsdp], null, null]
